[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/DLC1.html",
    "href": "posts/DLC1.html",
    "title": "딥러닝 Course1 by Andrew ng",
    "section": "",
    "text": "X(입력)와 Y(출력)를 연결지어주는 함수를 찾는 과정\n데이터가 많으면 많을수록 성능이 좋은 함수를 찾을 수 있음\n해당 뉴런에 관련 없는 입력값이라도 입력해야 함\n그 입력의 관계 여부, 가중치는 학습하면서 조절됨\n\n\n\n\n\nNN: 데이터베이스화된 데이터에 적합\nCNN: 이미지에 적함\nRNN: 오디오, 텍스트에 적합\n\n\n\n\n\n정형 데이터\n\n데이터베이스로 표현 가능\n정보의 특성 확정\n\n비정형 데이터\n\n오디오, 텍스트, 이미지\n특징값을 추출하기 어려움\n딥러닝 기술 발전으로 판별 가능\n\n\n\n\n\n\n디지털 정보량의 증가, 컴퓨터 성능 향상, 알고리즘 혁신\nSigmoid –&gt; ReLU로 activation function을 바꾸어 학습 속도 향상"
  },
  {
    "objectID": "posts/DLC1.html#c1w1",
    "href": "posts/DLC1.html#c1w1",
    "title": "딥러닝 Course1 by Andrew ng",
    "section": "",
    "text": "X(입력)와 Y(출력)를 연결지어주는 함수를 찾는 과정\n데이터가 많으면 많을수록 성능이 좋은 함수를 찾을 수 있음\n해당 뉴런에 관련 없는 입력값이라도 입력해야 함\n그 입력의 관계 여부, 가중치는 학습하면서 조절됨\n\n\n\n\n\nNN: 데이터베이스화된 데이터에 적합\nCNN: 이미지에 적함\nRNN: 오디오, 텍스트에 적합\n\n\n\n\n\n정형 데이터\n\n데이터베이스로 표현 가능\n정보의 특성 확정\n\n비정형 데이터\n\n오디오, 텍스트, 이미지\n특징값을 추출하기 어려움\n딥러닝 기술 발전으로 판별 가능\n\n\n\n\n\n\n디지털 정보량의 증가, 컴퓨터 성능 향상, 알고리즘 혁신\nSigmoid –&gt; ReLU로 activation function을 바꾸어 학습 속도 향상"
  },
  {
    "objectID": "posts/DLC1.html#c1w2",
    "href": "posts/DLC1.html#c1w2",
    "title": "딥러닝 Course1 by Andrew ng",
    "section": "C1W2",
    "text": "C1W2\n\n신경망 학습방법\n\n정방향 전파\n역방향 전파\n\n\n\nBinary Classification(이진 분류)\n\n1 or 0로 분류하는 것\n연체를 했다 / 연체를 하지 않았다.\n로지스틱 회귀(Logistic regression) 알고리즘 사용\n\n\n\n표기\n\n\\(m\\): 학습을 위한 데이터 세트 수\n\\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... (x^{(m)}, y^{(m)}), }\\)\n\\(n\\): 입력 데이터 하나의 원소 개수\n\\(x\\): 입력 데이터 하나\n\\(X\\): 입력 데이터\n\\(X = \\begin{bmatrix}\nx^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx^{(1)}_n & x^{(2)}_n & \\cdots & x^{(m)}_n\n\\end{bmatrix}\\)\nX.shape = (n, m)\n\\(y\\): 출력 데이터 하나\n\\(Y\\): 출력 데이터\n\\(Y = [y^{(1)}, y^{(2)}, ... y^{(n)}]\\)\nY.shape = (1, m)\n\\(\\hat{y}\\): 예측값\n0과 1 사이의 확률값으로 나타남\n\n\n\nLogistic regression(로지스틱 회귀)\n\n\n입력 특성(x)에 대한 실제값(y)을 가지고 예측값(\\(\\hat{y}\\))을 구하고 그 예측값과 실제값의 오차가 최소가 되도록 하는 파라미터(\\(W\\), \\(b\\))를 구해야 함\n\\(W\\): \\(x\\)와 크기가 같은 \\(n\\)차원의 벡터\n\\(b\\): 상수\n예측값은 아래와 같이 구함\n\\(\\hat{y} = \\sigma(W^TX+b)\\)\n\n\n\nSigmoid function\n\n\\(\\sigma(z) = {1 \\over 1 + e^{-z}}\\)\n\n\\(z\\)가 클 수록 1로 수렴\n\\(z\\)가 작을 수록 0으로 수렴\n\n위 식에서 \\(\\sigma\\)가 하는 역할은 예측값이 0에서 1사이가 되도록 만드는 역할\nSigmoid 함수 그래프\n\n\n\n\\(\\hat{y}\\)은 항상 0에서 1사이의 값을 가진다\n\n\n\nLoss Function(손실 함수)\n\n한 세트에 대한 예측값(\\(\\hat{y}\\))과 실제값(\\(y\\))의 오차를 구하는 함수\n\\(L(y, \\hat{y}) = -(y\\log{\\hat{y}}+(1-y)\\log(1-\\hat{y}))\\)\n실제값(\\(y\\))이 0이냐 1이냐에 따라서 오차를 구하는 식이 달라진다.\n\\(y=0\\)일 때: \\(L(y, \\hat{y}) = -y\\log{(1-\\hat{y})}\\)\n\\(y=1\\)일 때: \\(L(y, \\hat{y}) = -y\\log{\\hat{y}}\\)\n그래프로 표현\n\n\n\n\nCost Function(비용 함수)\n\n모든 입력세트에 대한 오차를 구하는 함수\n\\(J(W, b) = -{1 \\over m}\\sum_{i=1}^m(y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}))\\)\n로지스틱 회귀 모델을 학습한다는 것은 비용 함수 \\(J\\)를 최소로 만드는 \\(W\\)와 \\(b\\)를 찾는 것을 의미\n\n\n\n경사 하강법\n\n비용함수의 값을 최소화하는 \\(w\\)와 \\(b\\)를 찾는데 사용할 수 있는 방법이다. 이때 비용함수는 볼록한(convex) 형태여야 한다. 만약 비용함수의 형태가 볼록하지 않다면 지역 최솟값을 여러 개 가지게 되어 진짜 최솟값을 찾기 어려워진다.\n비용함수의 최솟값을 찾기 위한 시작점은 임의로 정하여도 상관없다. 경사 하강법을 사용하면 어디에서 시작하든 최솟값이 있는 곳으로 향하게 된다. 가파른 방향으로 한 스텝씩 업데이트하며 최솟값을 찾아간다.\n\\(w:=w-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}\\)\n\\(b:=b-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}\\)\n\n\n\n로지스틱 회귀의 경사 하강법에서 for 문이 알고리즘을 비효율적으로 만듬 -&gt; 벡터화를 통해 명시적 for 문을 제거\n\n\n\n벡터화\n\n벡터화를 사용하여 동시에 분산 처리\n\n\nimport numpy as np\nimport time\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\n\ntic = time.time()\nc = np.dot(a, b)\ntoc = time.time()\n\nprint(c)\nprint(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")\n\n250003.57277653238\nVectorized version: 0.49757957458496094ms\n\n\n\nfor 문을 사용해서 순차적으로 계산\n\n\nc = 0\ntic = time.time()\nfor i in range(1000000):\n    c += a[i]*b[i]\ntoc = time.time()\n\nprint(c)\nprint(\"for loop: \" + str(1000*(toc-tic)) + \"ms\")\n\n250003.5727765315\nfor loop: 353.87134552001953ms\n\n\n\n즉 벡터화를 사용한 코드의 시간이 훨씬 빠르다(0.49ms&lt;&lt;353.87ms)\n\n\n\n파이썬 브로드캐스팅\n\n각 식자재 100g당 영양소가 가지는 칼로리 표\n\n\n\n\n\nApples\nBeef\nEggs\nPotatoes\n\n\n\n\nCarb\n56.0\n0.0\n4.4\n68.0\n\n\nProtein\n1.2\n104.0\n52.0\n8.0\n\n\nFat\n1.8\n135.0\n99.0\n0.9\n\n\n\n\n브로딩캐스팅을 이용해 식자재 총 칼로리 중 각 영양소가 차지하는 비율을 구하는 예제\n\n\nimport numpy as np\n\nA = np.array([[56, 0, 4.4, 68],\n[1.2, 104, 52, 8],\n[1.8, 135, 99, 0.9]])\n\ncal = np.sum(A, axis=0)\npercentage = 100*A/cal.reshape(1, 4)\nprint(percentage)\n\n[[94.91525424  0.          2.83140283 88.42652796]\n [ 2.03389831 43.51464435 33.46203346 10.40312094]\n [ 3.05084746 56.48535565 63.70656371  1.17035111]]"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep_Learning",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n딥러닝 Course1 by Andrew ng\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learnig\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\n유성준\n\n\n\n\n\n\nNo matching items"
  }
]