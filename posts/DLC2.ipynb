{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Course2 Part1 by Andrew ng\"\n",
    "author: \"유성준\"\n",
    "date: \"01/17/2024\"\n",
    "categories: [Machine Learning, Deep Learnig, Python] \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망 초기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 신경망을 훈련시킬 때 결정해야하는 것들은 다음과 같이 매우 많다.\n",
    "\n",
    "- 층의 수(number of layers)\n",
    "- 뉴런의 수(number of hidden units)\n",
    "- 학습률(learning rate)\n",
    "- 활성화 함수(Activation Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 처음부터 적절한 값을 추정하는 것은 거의 불가능, 다른 hyper parameters도 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래서 머신러닝은 매우 반복적인 과정을 거쳐야한다.\n",
    "\n",
    "1) 처음에는 아이디어로 시작한다.\r\n",
    "2) 그리고 특정 개수의 층과 유닛을 가지고 특정 데이터 셋에 맞는 신경망을 만든다.\r\n",
    "3) 이를 코드로 작성하고 실행하고 성능을 검사한다.\r\n",
    "4) 그 결과에 기반하여 아이디어를 개선하고 몇 가지 선택을 수정하게 된다. 그리고 더 나은 신경망을 찾기 위해 이 과정을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 훈련, 개발, 테스트 데이터 셋을 어떻게 설정할지에 따라 좋은 모델을 빠르게 찾는데 큰 영향을 준다. 데이터 셋을 나눌 때 데이터의 수가 적다면 전통적인 70/30이나 60/20/20 으로 비율을 설정해도 괜찮지만 훨씬 데이터의 수가 많다면 개발과 테스트 데이터 셋의 비율을 더 적게 설정하는 것도 좋은 방법이다.(98/1/1 or 99/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png \" alt=\"이미지\" width=\"600\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 높은 편향(high bias) &rarr; under-fitting\n",
    "- 높은 분산(high variance) &rarr; overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.oreilly.com/api/v2/epubs/9781491924570/files/assets/dpln_0107.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` train set error와 dev set error의 변화량에서 분산과 편향 문제에 대한 정도를 알수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training set error | dev set error |      |\n",
    "|------------|--------|-------|\n",
    "| 15%   | 16%   | high bias-underfitting   |\n",
    "| 0.5%  | 1%    | low bias, low variance-good fitting |\n",
    "| 1%    | 11%   | high variance-overfitting |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 분산과 편향을 잘 확인해서 현재 모델을 진단하고 적절한 수정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) high bias\n",
    "\n",
    "`-` 해결방법\n",
    "\n",
    "- (더 많은 은닉층 혹은 은닉 유닛을 갖는) 큰 네트워크 선택\r",
    "- Train longer or 더 발전된 최적화 알고리즘 사용\n",
    "- \n",
    "다른 신경망 아키텍쳐 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) high variance\n",
    "\n",
    "`-` 해결방법\n",
    "\n",
    "- 더 많은 데이터 확보\n",
    "- Regularization(정규화)\n",
    "- 다른 신경망 아키텍쳐 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 과거에는 편향-분산 트레이드오프가 중요한 요소였지만 현대의 딥러닝 빅데이터 시대에는 더 큰 네트워크로 인해 편향과 분산이 서로 거의 영향을 끼치지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 높은 분산으로 Overfitting이 일어났을때 해결 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 더 많은 훈련 데이터를 얻는다. &rarr; 비용이 많이 든다.\n",
    "- 그래서 정규화를 시도해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $J(W, b) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i})+{\\lambda \\over 2m}\\| w \\|^2_2++{\\lambda \\over 2m}b^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $J(W_1, b_1,...,W_L, b_L) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i}) + {\\lambda \\over 2m}\\sum_l\\sum_i\\sum_j\\| w^{[l]}_{ij} \\|^2_F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규화의 종류에는 L1 정규화와 L2 정규화가 있고 대부분 L2 정규화를 많이 사용한다\n",
    "- 정규화를 하게 되면 가중치($w$) 값들이 작아지게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결론적으로, 높은 분산 문제가 발생하였을 경우 더 많은 훈련 데이터 셋을 확보할 수 없다면 정규화를 시도해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 과적합을 줄이기 위해 왜 정규화를 하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.oreilly.com/api/v2/epubs/9781491924570/files/assets/dpln_0107.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting이 발생하는 원인은 네트워크가 너무 커서 매우 복잡한 비선형 함수를 만들기 때문이다.\n",
    "- 정규화는 이를 해결하여 Overfitting의 가능성을 줄여줄 수 있다.\n",
    "- 정규화를 하면 아래식의 파란색 부분(L2 정규화 항)을 더해주게 된다.\n",
    "- $\\color{red}\\lambda$는 정규화 매개변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $J(W_1, b_1,...,W_L, b_L) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i}) + \\color{blue}{\\color{red}\\lambda \\over 2m}\\sum_l\\sum_i\\sum_j\\| w^{[l]}_{ij} \\|^2_F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 페널티를 추가하면 일부 뉴런의 영향이 작아지거나 없어져서 모델이 더 단순해진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 직관적인 예시:tanh(z) 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$가 커질때 비용 함수가 커지지 않으려면 상대적으로 $w$가 작아진다\n",
    "- $w$가 작으면 $z$도 상대적으로 작은값을 가지게 된다\n",
    "- $z$가 상대적으로 작은값을 갖게 되면 $tanh(z)$는 0 근처에서 거의 1차원 함수가 된다\n",
    "- 따라서 모든 층은 선형 회귀처럼 거의 직선의 함수를 갖게 된다.\n",
    "- 모든 층이 선형이면 전체 네트워크도 선형이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{[l]} = w^{[l]}*a^{[l-1]} + b^{l}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2Ffc03f0c3-9b61-4138-b008-1ace026a4262%2Fimage.png\" alt=\"이미지\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 정규화를 하게 되면 정규화 매개변수를 포함한 추가적인 항을 손실함수에 더하게 되고 그럼 결국 가중치($w$)가 작아져 은닉층의 유닛들의 영향력이 작아지고 네트워크의 복잡도가 줄어든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 과적합한 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2F1cc4dba1-ff15-4132-a2c0-34562c85088f%2FSmartSelect_20210124-183809_Samsung%20Notes.jpg\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50% 확률로 각 층에서 노드를 삭제한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2Fbf47534f-d06b-4efe-b65c-3a1270366cd8%2FSmartSelect_20210124-183248_YouTube.jpg\" width=\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 간소화된 네트워크에서 _역전파_ 도 진행한다. 노드를 무작위로 삭제하는 것이 이상한 기법처럼 보일 수도 있지만 실제로 잘 작동한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 역전파란?\n",
    "\n",
    "- 신경망의 학습 알고리즘 중 하나로, 네트워크의 출력에서 시작하여 입력 방향으로 오차를 다시 전파하며 가중치를 조정하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모델 훈련 후 테스트할 때 테스트 데이터 셋의 경우에는 드롭아웃을 사용하지 않는다. 왜냐하면 테스트를 할 때 결과가 무작위로 나오면 안되기 때문이다.<br>\n",
    "오히려 테스트에서 드롭아웃을 사용하면 오히려 노이즈가 증가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Dropout이 정규화의 효과가 나는 이유는?\n",
    "\n",
    "1. 더 작고 간단한 신경말을 사용하게 되면서 정규화의 효과를 주게 된다.\n",
    "2. 단일 유닛의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이미지2](https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2Fc4cc167e-7faa-4fda-9b7b-a1e10191be26%2Fimage.png){width=50%}\n",
    "![이미지3](https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2F22eb0d64-ca58-4ed5-99f2-6d71283d084f%2Fimage.png){width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런 식으로 드롭아웃을 통해 무작위로 유닛들이 삭제되어 서로 다른 다양한 input을 오른쪽의 단일 유닛은 전달 받게 된다.\n",
    "- 따라서 오른쪽의 단일 유닛은 어떠한 입력 특성에도 의존할 수 없다. 하나의 특정 입력에 유난히 큰 가중치를 부여할 수 없으므로 가중치를 4개의 input에 각각 분산시킨다.\n",
    "- 즉, 가중치를 분산시킴으로써 가중치의 Norm(L2 정규화)이 줄어들게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dropout도 정규화 기법이고 Overfitting을 막는데 도움을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Other Regularization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습을 위해 새로운 이미지가 많으면 많을수록 좋지만 비용적인 한계가 있다.\n",
    "- 그래서 하나의 미지를 수평 방향으로 뒤집거나 회전, 왜곡 및 변형을 하여 샘플에 추가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.velog.io/images/jhlee508/post/5c7ba8ea-d4c5-41c3-89cc-df060494b0a3/image.png\" alt=\"이미지\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training set error와 dev set error의 그래프\n",
    "- dev set error 가 증가하는 순간에 학습을 멈춘다.\n",
    "- 조기 종료를 하기에 비용함수를 잘 줄이지 못한다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://yakout.io/assets/images/dl/early-stopping.png\" alt=\"이미지\" width=\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 정규화 방법들은 모두 각각의 장점과 단점을 가지고 있다. Overfitting을 피하기 위해선 상황을 잘 고려하고 좋은 결과값이 나오는 정규화 방법을 택해야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
