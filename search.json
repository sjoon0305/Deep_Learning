[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/DLC2.html",
    "href": "posts/DLC2.html",
    "title": "Course2 Part1 by Andrew ng",
    "section": "",
    "text": "훈련, 개발, 테스트 데이터 셋을 어떻게 설정할지에 관한 좋은 선택을 내리는 것은 좋은 성능을 내는 네트워크를 빠르게 찾는데 큰 영향을 준다.\n- 신경망을 훈련시킬 때 결정해야하는 것들은 다음과 같이 매우 많다.\n\n층의 수(number of layers)\n뉴런의 수(number of hidden units)\n학습률(learning rate)\n활성화 함수(Activation Function)\n\n- 처음부터 적절한 값을 추정하는 것은 거의 불가능, 다른 hyper parameters도 해당 - 그래서 머신러닝은 매우 반복적인 과정을 거쳐야한다.\n\n처음에는 아이디어로 시작한다.\n그리고 특정 개수의 층과 유닛을 가지고 특정 데이터 셋에 맞는 신경망을 만든다.\n이를 코드로 작성하고 실행하고 성능을 검사한다.\n그 결과에 기반하여 아이디어를 개선하고 몇 가지 선택을 수정하게 된다. 그리고 더 나은 신경망을 찾기 위해 이 과정을 반복한다.\n\n\n좋은 모델을 만드는 요소는 이 사이클을 얼마나 효율적으로 돌아갈 수 있는지와 데이터 셋을 잘 설정하는 것이다. 훈련, 개발, 테스트 셋을 잘 설정하는 것은 과정을 더 효율적으로 만든다.\n\n\n결론적으로, 데이터 셋을 나눌 때 데이터의 수가 적다면 전통적인 70/30이나 60/20/20 으로 비율을 설정해도 괜찮지만 훨씬 데이터의 수가 많다면 개발과 테스트 데이터 셋의 비율을 더 적게 설정하는 것도 좋은 방법이다."
  },
  {
    "objectID": "posts/DLC2.html#traindevtest-sets",
    "href": "posts/DLC2.html#traindevtest-sets",
    "title": "Course2 Part1 by Andrew ng",
    "section": "",
    "text": "훈련, 개발, 테스트 데이터 셋을 어떻게 설정할지에 관한 좋은 선택을 내리는 것은 좋은 성능을 내는 네트워크를 빠르게 찾는데 큰 영향을 준다.\n- 신경망을 훈련시킬 때 결정해야하는 것들은 다음과 같이 매우 많다.\n\n층의 수(number of layers)\n뉴런의 수(number of hidden units)\n학습률(learning rate)\n활성화 함수(Activation Function)\n\n- 처음부터 적절한 값을 추정하는 것은 거의 불가능, 다른 hyper parameters도 해당 - 그래서 머신러닝은 매우 반복적인 과정을 거쳐야한다.\n\n처음에는 아이디어로 시작한다.\n그리고 특정 개수의 층과 유닛을 가지고 특정 데이터 셋에 맞는 신경망을 만든다.\n이를 코드로 작성하고 실행하고 성능을 검사한다.\n그 결과에 기반하여 아이디어를 개선하고 몇 가지 선택을 수정하게 된다. 그리고 더 나은 신경망을 찾기 위해 이 과정을 반복한다.\n\n\n좋은 모델을 만드는 요소는 이 사이클을 얼마나 효율적으로 돌아갈 수 있는지와 데이터 셋을 잘 설정하는 것이다. 훈련, 개발, 테스트 셋을 잘 설정하는 것은 과정을 더 효율적으로 만든다.\n\n\n결론적으로, 데이터 셋을 나눌 때 데이터의 수가 적다면 전통적인 70/30이나 60/20/20 으로 비율을 설정해도 괜찮지만 훨씬 데이터의 수가 많다면 개발과 테스트 데이터 셋의 비율을 더 적게 설정하는 것도 좋은 방법이다."
  },
  {
    "objectID": "posts/DLC2.html#biasvariance",
    "href": "posts/DLC2.html#biasvariance",
    "title": "Course2 Part1 by Andrew ng",
    "section": "2. Bias/Variance",
    "text": "2. Bias/Variance\n\n\n높은 편향(high bias) → under-fitting\n높은 분산(high variance) → overfitting\n\n\n- train set error와 dev set error의 변화량에서 분산과 편향 문제에 대한 정도를 알수 있다\n\n\n\nTraining set error\ndev set error\n\n\n\n\n\n15%\n16%\nhigh bias-underfitting\n\n\n0.5%\n1%\nlow bias, low variance-good fitting\n\n\n1%\n11%\nhigh variance-overfitting\n\n\n\n\n분산과 편향을 잘 확인해서 현재 모델을 진단하고 적절한 수정을 거쳐야 한다."
  },
  {
    "objectID": "posts/DLC2.html#basic-recipe-for-machine-learning",
    "href": "posts/DLC2.html#basic-recipe-for-machine-learning",
    "title": "Course2 Part1 by Andrew ng",
    "section": "3. Basic Recipe for Machine Learning",
    "text": "3. Basic Recipe for Machine Learning\n\n편향 확인-training data set error 확인\n\n- 해결방법\n\n(더 많은 은닉층 혹은 은닉 유닛을 갖는) 큰 네트워크 선택- Train longer or 더 발전된 최적화 알고리즘 사\n다른 신경망 아키텍쳐 사용\n\n\n분산 확인-Dev data set 성능 확인\n\n- 해결방법\n\n더 많은 데이터 확보- Regularization(정규화\n다른 신경망 아키텍쳐 사용\n\n\n과거에는 편향-분산 트레이드오프가 중요한 요소였지만 현대의 딥러닝 빅데이터 시대에는 더 큰 네트워크로 인해 편향과 분산이 서로 거의 영향을 끼치지 않는다."
  },
  {
    "objectID": "posts/DLC2.html#regularization",
    "href": "posts/DLC2.html#regularization",
    "title": "Course2 Part1 by Andrew ng",
    "section": "4. Regularization",
    "text": "4. Regularization\n높은 분산으로 Overfitting이 일어났을때 - 더 많은 훈련 데이터를 얻는다. → 비용이 많이 든다. - 그래서 정규화를 시도해본다.\n\nL2 Regularization\n- Logistic Regression\n\n\\(J(W, b) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i})+{\\lambda \\over 2m}\\| w \\|^2_2++{\\lambda \\over 2m}b^2\\)\n\n- Neural Networks\n\n\\(J(W_1, b_1,...,W_L, b_L) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i}) + {\\lambda \\over 2m}\\sum_l\\sum_i\\sum_j\\| w^{[l]}_{ij} \\|^2_F\\)\n정규화의 종류에는 L1 정규화와 L2 정규화가 있고 대부분 L2 정규화를 많이 사용한다\n정규화를 하게 되면 가중치(\\(w\\)) 값들이 작아지게 된다\n\n\n결론적으로, 높은 분산 문제가 발생하였을 경우 더 많은 훈련 데이터 셋을 확보할 수 없다면 정규화를 시도해볼 수 있다."
  },
  {
    "objectID": "posts/DLC2.html#과적합을-줄이기-위해-왜-정규화를-하는가",
    "href": "posts/DLC2.html#과적합을-줄이기-위해-왜-정규화를-하는가",
    "title": "Course2 Part1 by Andrew ng",
    "section": "5. 과적합을 줄이기 위해 왜 정규화를 하는가?",
    "text": "5. 과적합을 줄이기 위해 왜 정규화를 하는가?\n\n\nOverfitting이 발생하는 원인은 네트워크가 너무 커서 매우 복잡한 비선형 함수를 만들기 때문이다.\n정규화는 이를 해결하여 Overfitting의 가능성을 줄여줄 수 있다.\n정규화를 하면 아래식의 파란색 부분(L2 정규화)을 더해주게 된다.\n\\(\\color{red}\\lambda\\)는 정규화 매개변수\n\\(J(W_1, b_1,...,W_L, b_L) = {1 \\over m}\\sum_{i=1}^m\\mathcal{L}(\\hat{y},y^{i}) + \\color{blue}{\\color{red}\\lambda \\over 2m}\\sum_l\\sum_i\\sum_j\\| w^{[l]}_{ij} \\|^2_F\\)\n이렇게 페널티를 추가하면 일부 뉴런의 영향이 작아지거나 없어져서 모델이 더 단순해진다.\n\n- 직관적인 예시\n\n\\(\\lambda\\)가 커질때 비용 함수가 커지지 않으려면 상대적으로 \\(w\\)가 작아진다\n\\(w\\)가 작으면 \\(z\\)도 상대적으로 작은값을 가지게 된다\n\\(z\\)가 상대적으로 작은값을 갖게 되면 \\(tanh(z)\\)는 0 근처에서 거의 1차원 함수가 된다\n따라서 모든 층은 선형 회귀처럼 거의 직선의 함수를 갖게 된다.\n모든 층이 선형이면 전체 네트워크도 선형이 된다.\n\n\\(z^{[l]} = w^{[l]}*a^{[l-1]} + b^{l}\\)\n\n\n정규화를 하게 되면 정규화 매개변수를 포함한 추가적인 항을 손실함수에 더하게 되고 그럼 결국 가중치(\\(w\\))가 작아져 은닉층의 유닛들의 영향력이 작아지고 네트워크의 복잡도가 줄어든다."
  },
  {
    "objectID": "posts/DLC2.html#dropout-regularization",
    "href": "posts/DLC2.html#dropout-regularization",
    "title": "Course2 Part1 by Andrew ng",
    "section": "6. Dropout Regularization",
    "text": "6. Dropout Regularization\n- 과적합한 신경망\n\n\n50% 확률로 각 층에서 노드를 삭제한다.\n\n\n\n이 간소화된 네트워크에서 역전파 도 진행한다. 노드를 무작위로 삭제하는 것이 이상한 기법처럼 보일 수도 있지만 실제로 잘 작동한다\n\n- 역전파란?\n\n신경망의 학습 알고리즘 중 하나로, 네트워크의 출력에서 시작하여 입력 방향으로 오차를 다시 전파하며 가중치를 조정하는 과정\n\n\n모델 훈련 후 테스트할 때 테스트 데이터 셋의 경우에는 드롭아웃을 사용하지 않는다. 왜냐하면 테스트를 할 때 결과가 무작위로 나오면 안되기 때문이다. 오히려 테스트에서 드롭아웃을 사용하면 오히려 노이즈가 증가한다"
  },
  {
    "objectID": "posts/DLC2.html#understanding-dropout",
    "href": "posts/DLC2.html#understanding-dropout",
    "title": "Course2 Part1 by Andrew ng",
    "section": "7. Understanding Dropout",
    "text": "7. Understanding Dropout\n- Dropout이 정규화의 효과가 나는 이유는?\n\n더 작고 간단한 신경말을 사용하게 되면서 정규화의 효과를 주게 된다.\n단일 유닛의 예시\n\n  \n\n이런 식으로 드롭아웃을 통해 무작위로 유닛들이 삭제되어 서로 다른 다양한 input을 오른쪽의 단일 유닛은 전달 받게 된다.\n따라서 오른쪽의 단일 유닛은 어떠한 입력 특성에도 의존할 수 없다. 하나의 특정 입력에 유난히 큰 가중치를 부여할 수 없으므로 가중치를 4개의 input에 각각 분산시킨다.\n즉, 가중치를 분산시킴으로써 가중치의 Norm(L2 정규화)이 줄어들게 된다.\n\n\nDropout은 정규화 기법이고 Overfitting을 막는데 도움을 준다."
  },
  {
    "objectID": "posts/DLC2.html#other-regularization-methods",
    "href": "posts/DLC2.html#other-regularization-methods",
    "title": "Course2 Part1 by Andrew ng",
    "section": "8. Other Regularization Methods",
    "text": "8. Other Regularization Methods\n\nData augmentation\n\n학습을 위해 새로운 이미지가 많으면 많을수록 좋지만 비용적인 한계가 있다.\n그래서 하나의 미지를 수평 방향으로 뒤집거나 회전, 왜곡 및 변형을 하여 샘플에 추가한다\n\n\n\n\nEarly Stopping\n\ntraining set error와 dev set error의 그래프\ndev set error 가 증가하는 순간에 학습을 멈춘다.\n조기 종료를 하기에 비용함수를 잘 줄이지 못한다는 단점이 있다.\n\n\n\n정규화 방법들은 모두 각각의 장점과 단점을 가지고 있다. Overfitting을 피하기 위해선 상황을 잘 고려하고 좋은 결과값이 나오는 정규화 방법을 택해야 한다."
  },
  {
    "objectID": "posts/DLC1.html",
    "href": "posts/DLC1.html",
    "title": "Course1 by Andrew ng",
    "section": "",
    "text": "X(입력)와 Y(출력)를 연결지어주는 함수를 찾는 과정\n데이터가 많으면 많을수록 성능이 좋은 함수를 찾을 수 있음\n해당 뉴런에 관련 없는 입력값이라도 입력해야 함\n그 입력의 관계 여부, 가중치는 학습하면서 조절됨\n\n\n\n\n\nNN: 데이터베이스화된 데이터에 적합\nCNN: 이미지에 적함\nRNN: 오디오, 텍스트에 적합\n\n\n\n\n\n정형 데이터\n\n데이터베이스로 표현 가능\n정보의 특성 확정\n\n비정형 데이터\n\n오디오, 텍스트, 이미지\n특징값을 추출하기 어려움\n딥러닝 기술 발전으로 판별 가능\n\n\n\n\n\n\n디지털 정보량의 증가, 컴퓨터 성능 향상, 알고리즘 혁신\nSigmoid –&gt; ReLU로 activation function을 바꾸어 학습 속도 향상"
  },
  {
    "objectID": "posts/DLC1.html#c1w1",
    "href": "posts/DLC1.html#c1w1",
    "title": "Course1 by Andrew ng",
    "section": "",
    "text": "X(입력)와 Y(출력)를 연결지어주는 함수를 찾는 과정\n데이터가 많으면 많을수록 성능이 좋은 함수를 찾을 수 있음\n해당 뉴런에 관련 없는 입력값이라도 입력해야 함\n그 입력의 관계 여부, 가중치는 학습하면서 조절됨\n\n\n\n\n\nNN: 데이터베이스화된 데이터에 적합\nCNN: 이미지에 적함\nRNN: 오디오, 텍스트에 적합\n\n\n\n\n\n정형 데이터\n\n데이터베이스로 표현 가능\n정보의 특성 확정\n\n비정형 데이터\n\n오디오, 텍스트, 이미지\n특징값을 추출하기 어려움\n딥러닝 기술 발전으로 판별 가능\n\n\n\n\n\n\n디지털 정보량의 증가, 컴퓨터 성능 향상, 알고리즘 혁신\nSigmoid –&gt; ReLU로 activation function을 바꾸어 학습 속도 향상"
  },
  {
    "objectID": "posts/DLC1.html#c1w2",
    "href": "posts/DLC1.html#c1w2",
    "title": "Course1 by Andrew ng",
    "section": "C1W2",
    "text": "C1W2\n\n신경망 학습방법\n\n정방향 전파\n역방향 전파\n\n\n\nBinary Classification(이진 분류)\n\n1 or 0로 분류하는 것\n연체를 했다 / 연체를 하지 않았다.\n로지스틱 회귀(Logistic regression) 알고리즘 사용\n\n\n\n표기\n\n\\(m\\): 학습을 위한 데이터 세트 수\n\\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... (x^{(m)}, y^{(m)}), }\\)\n\\(n\\): 입력 데이터 하나의 원소 개수\n\\(x\\): 입력 데이터 하나\n\\(X\\): 입력 데이터\n\\(X = \\begin{bmatrix}\nx^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx^{(1)}_n & x^{(2)}_n & \\cdots & x^{(m)}_n\n\\end{bmatrix}\\)\nX.shape = (n, m)\n\\(y\\): 출력 데이터 하나\n\\(Y\\): 출력 데이터\n\\(Y = [y^{(1)}, y^{(2)}, ... y^{(n)}]\\)\nY.shape = (1, m)\n\\(\\hat{y}\\): 예측값\n0과 1 사이의 확률값으로 나타남\n\n\n\nLogistic regression(로지스틱 회귀)\n\n\n입력 특성(x)에 대한 실제값(y)을 가지고 예측값(\\(\\hat{y}\\))을 구하고 그 예측값과 실제값의 오차가 최소가 되도록 하는 파라미터(\\(W\\), \\(b\\))를 구해야 함\n\\(W\\): \\(x\\)와 크기가 같은 \\(n\\)차원의 벡터\n\\(b\\): 상수\n예측값은 아래와 같이 구함\n\\(\\hat{y} = \\sigma(W^TX+b)\\)\n\n\n\nSigmoid function\n\n\\(\\sigma(z) = {1 \\over 1 + e^{-z}}\\)\n\n\\(z\\)가 클 수록 1로 수렴\n\\(z\\)가 작을 수록 0으로 수렴\n\n위 식에서 \\(\\sigma\\)가 하는 역할은 예측값이 0에서 1사이가 되도록 만드는 역할\nSigmoid 함수 그래프\n\n\n\n\\(\\hat{y}\\)은 항상 0에서 1사이의 값을 가진다\n\n\n\nLoss Function(손실 함수)\n\n한 세트에 대한 예측값(\\(\\hat{y}\\))과 실제값(\\(y\\))의 오차를 구하는 함수\n\\(L(y, \\hat{y}) = -(y\\log{\\hat{y}}+(1-y)\\log(1-\\hat{y}))\\)\n실제값(\\(y\\))이 0이냐 1이냐에 따라서 오차를 구하는 식이 달라진다.\n\\(y=0\\)일 때: \\(L(y, \\hat{y}) = -y\\log{(1-\\hat{y})}\\)\n\\(y=1\\)일 때: \\(L(y, \\hat{y}) = -y\\log{\\hat{y}}\\)\n그래프로 표현\n\n\n\n\nCost Function(비용 함수)\n\n모든 입력세트에 대한 오차를 구하는 함수\n\\(J(W, b) = -{1 \\over m}\\sum_{i=1}^m(y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}))\\)\n로지스틱 회귀 모델을 학습한다는 것은 비용 함수 \\(J\\)를 최소로 만드는 \\(W\\)와 \\(b\\)를 찾는 것을 의미\n\n\n\n경사 하강법\n\n비용함수의 값을 최소화하는 \\(w\\)와 \\(b\\)를 찾는데 사용할 수 있는 방법이다. 이때 비용함수는 볼록한(convex) 형태여야 한다. 만약 비용함수의 형태가 볼록하지 않다면 지역 최솟값을 여러 개 가지게 되어 진짜 최솟값을 찾기 어려워진다.\n비용함수의 최솟값을 찾기 위한 시작점은 임의로 정하여도 상관없다. 경사 하강법을 사용하면 어디에서 시작하든 최솟값이 있는 곳으로 향하게 된다. 가파른 방향으로 한 스텝씩 업데이트하며 최솟값을 찾아간다.\n\\(w:=w-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}\\)\n\\(b:=b-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}\\)\n\n\n\n로지스틱 회귀의 경사 하강법에서 for 문이 알고리즘을 비효율적으로 만듬 -&gt; 벡터화를 통해 명시적 for 문을 제거\n\n\n\n벡터화\n\n벡터화를 사용하여 동시에 분산 처리\n\n\nimport numpy as np\nimport time\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\n\ntic = time.time()\nc = np.dot(a, b)\ntoc = time.time()\n\nprint(c)\nprint(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")\n\n250003.57277653238\nVectorized version: 0.49757957458496094ms\n\n\n\nfor 문을 사용해서 순차적으로 계산\n\n\nc = 0\ntic = time.time()\nfor i in range(1000000):\n    c += a[i]*b[i]\ntoc = time.time()\n\nprint(c)\nprint(\"for loop: \" + str(1000*(toc-tic)) + \"ms\")\n\n250003.5727765315\nfor loop: 353.87134552001953ms\n\n\n\n즉 벡터화를 사용한 코드의 시간이 훨씬 빠르다(0.49ms&lt;&lt;353.87ms)\n\n\n\n파이썬 브로드캐스팅\n\n각 식자재 100g당 영양소가 가지는 칼로리 표\n\n\n\n\n\nApples\nBeef\nEggs\nPotatoes\n\n\n\n\nCarb\n56.0\n0.0\n4.4\n68.0\n\n\nProtein\n1.2\n104.0\n52.0\n8.0\n\n\nFat\n1.8\n135.0\n99.0\n0.9\n\n\n\n\n브로딩캐스팅을 이용해 식자재 총 칼로리 중 각 영양소가 차지하는 비율을 구하는 예제\n\n\nimport numpy as np\n\nA = np.array([[56, 0, 4.4, 68],\n[1.2, 104, 52, 8],\n[1.8, 135, 99, 0.9]])\n\ncal = np.sum(A, axis=0)\npercentage = 100*A/cal.reshape(1, 4)\nprint(percentage)\n\n[[94.91525424  0.          2.83140283 88.42652796]\n [ 2.03389831 43.51464435 33.46203346 10.40312094]\n [ 3.05084746 56.48535565 63.70656371  1.17035111]]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep_Learning",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 15, 2024\n\n\nCourse2 by Andrew ng\n\n\n유성준 \n\n\n\n\nJan 4, 2024\n\n\n딥러닝 Course1 by Andrew ng\n\n\n유성준 \n\n\n\n\n\nNo matching items"
  }
]